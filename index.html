<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="style/thang.css" type="text/css" />
<title>Thang Bui</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Thang Bui</h1>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>I'm looking for PhD/MPhil students. If you have a strong background in statistics and computer science, feel free to get in touch. Please see my research interests and publications below, and my email address at the bottom of this page.</p>
</div></div>
<h2>Bio</h2>
<p>I'm a lecturer (equivalent to tenure-track Assistant Professor) in Machine Learning and Data Science at the University of Sydney, Australia since 2018.</p>
<p>I spent two years (2019-2020) at Uber AI, working on probabilistic inference, continual learning, reinforcement learning and applications of these topics for marketplace experimentation and optimisation.<br /></p>
<p>I completed my doctoral training at the <a href="http://mlg.eng.cam.ac.uk">Cambridge Machine Learning group</a>, supervised by <a href="http://cbl.eng.cam.ac.uk/Public/Turner/WebHome">Richard Turner</a> and advised by <a href="http://learning.eng.cam.ac.uk/carl/">Carl Rasmussen</a>.<br /><br /> </p>
<p>My current research interests include probabilistic modelling and inference, Monte Carlo and approximate inference methods, distributed, active and continual leaning, and model-based reinforcement learning.<br /></p>
<h2>Preprints</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1811.11206">Partitioned Variational Inference: A unified framework encompassing federated and continual learning</a><br /> Thang Bui, Cuong Nguyen, Siddharth Swaroop, and Rich Turner</p>
</li>
</ul>
<h2>Conference and journal papers</h2>
<ul>
<li><p>q-Paths: Generalizing the geometric annealing path using power means<br /> Vaden Masrani, Rob Brekelmans, Thang Bui, Frank Nielsen , Aram Galstyan, Greg Ver Steeg, and Frank Wood <br /> UAI 2021</p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.05468">Variational autoregressive Gaussian processes for continual learning</a><br /> Sanyam Kapoor, Theofanis Karaletsos, and Thang Bui <br /> ICML 2021 <br /> <a href="https://github.com/uber-research/vargp">code</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/2002.04033">Hierarchical Gaussian process priors for Bayesian neural network weights</a><br /> Theofanis Karaletsos and Thang Bui <br /> NeurIPS 2020</p>
</li>
<li><p><a href="https://arxiv.org/abs/1710.10628">Variational continual learning</a><br /> Cuong Nguyen, Yingzhen Li, Thang Bui, and Rich Turner, <br /> ICLR 2018 <br /> <a href="https://github.com/nvcuong/variational-continual-learning">code</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1703.04818">Neural graph learning: Training neural networks using graphs</a><br /> Thang Bui, Sujith Ravi, and Vivek Ramavajjala <br /> WSDM 2018</p>
</li>
<li><p><a href="https://arxiv.org/abs/1605.07066">A unifying framework for sparse Gaussian process approximations using Power Expectation Propagation</a><br /> Thang Bui, Josiah Yan, Rich Turner <br /> JMLR 2017 <br /> <a href="https://github.com/thangbui/sparseGP_powerEP/">code</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1705.07131">Streaming sparse Gaussian process approximations</a><br /> Thang Bui, Cuong Nguyen, and Rich Turner <br /> NIPS 2017 <br /> <a href="https://github.com/thangbui/streaming_sparse_gp">code</a></p>
</li>
<li><p><a href="http://jmlr.org/proceedings/papers/v48/bui16.pdf">Deep Gaussian processes for regression using approximate Expectation Propagation</a><br /> Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, and Rich Turner <br /> ICML 2016 <br /> <a href="https://github.com/thangbui/deepGP_approxEP/">code</a></p>
</li>
<li><p><a href="http://jmlr.org/proceedings/papers/v48/hernandez-lobatob16.pdf">Black-box alpha-divergence minimization</a><br /> José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang Bui, and Rich Turner <br /> ICML 2016 <br /> <a href="https://bitbucket.org/jmh233/code_black_box_alpha_icml_2016">code</a></p>
</li>
<li><p><a href="https://papers.nips.cc/paper/5772-learning-stationary-time-series-using-gaussian-processes-with-nonparametric-kernels">Learning stationary time series using Gaussian processes with nonparametric kernels</a> <br /> Felipe Tobar, Thang Bui, and Rich Turner <br /> NIPS 2015 (<b>Spotlight</b>, acceptance rate = 3.6%)</p>
</li>
<li><p><a href="https://papers.nips.cc/paper/5459-tree-structured-gaussian-process-approximations">Tree-structured Gaussian process approximations</a><br /> Thang Bui and Rich Turner <br /> NIPS 2014 (<b>Spotlight</b>, acceptance rate = 3.6%) <br /> <a href="https://github.com/thangbui/tsgp/">code</a></p>
</li>
</ul>
<h2>Thesis</h2>
<ul>
<li><p><a href="docs/papers/thesis-thang.pdf">Efficient Deterministic Approximate Bayesian Inference for Gaussian Process Models</a> <br /> PhD thesis <br /> University of Cambridge <br /> <a href="docs/papers/thesis-thang-typos.txt">typos in the print version</a> </p>
</li>
</ul>
<h2>Workshop papers</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.07823">Annealed importance sampling with q-paths</a><br /> Rob Brekelmans, Vaden Masrani, Thang Bui, Frank Wood, Aram Galstyan, Greg Ver Steeg, and Frank Nielsen, <br /> NeurIPS workshop on Deep Learning through Information Geometry, 2020<br /> <b>Best paper award</b></p>
</li>
<li><p><a href="https://arxiv.org/abs/2006.05468">Variational autoregressive Gaussian processes for continual learning</a> <br /> Sanyam Kapoor, Theofanis Karaletsos, and Thang Bui <br /> ICML workshop on Continual Learning, 2020</p>
</li>
<li><p><a href="https://arxiv.org/abs/2002.04033">Gaussian process meta-representations for hierarchical neural network priors</a><br /> Theofanis Karaletsos and Thang Bui <br /> 2nd Symposium on Advances in Approximate Bayesian Inference, 2019</p>
</li>
<li><p><a href="https://arxiv.org/abs/1811.11206">Partitioned variational inference for federated Bayesian deep learning</a><br /> Thang Bui, Cuong Nguyen, Siddharth Swaroop, and Rich Turner <br /> NeurIPS Bayesian Deep Learning Workshop, 2018</p>
</li>
<li><p><a href="https://arxiv.org/abs/1905.02099">Understanding and improving variational continual learning</a><br /> Siddharth Swaroop, Cuong Nguyen, Thang Bui, and Rich Turner <br /> NeurIPS Continual Learning Workshop, 2018</p>
</li>
<li><p><a href="https://hanna-tseran.github.io/files/NeurIPS_Continual_Learning_Workshop_2018_Paper.pdf">Natural variational continual learning</a><br /> Hanna Tseran, Emtiyaz Khan, Tatsuya Harada and Thang Bui <br /> NeurIPS Continual Learning Workshop, 2018</p>
</li>
<li><p><a href="https://arxiv.org/abs/1710.10628">Variational continual learning for deep models</a><br /> Cuong Nguyen, Yingzhen Li, Thang Bui, and Rich Turner <br /> NIPS Bayesian Deep Learning Workshop, 2017</p>
</li>
<li><p><a href="http://roseyu.com/time-series-workshop/submissions/TSW2017_paper_7.pdf">Online variational Bayesian inference: Algorithms for sparse Gaussian processes and theoretical bounds</a><br /> Cuong Nguyen, Thang Bui, Yingzhen Li, and Rich Turner <br /> ICML Time Series Workshop, 2017</p>
</li>
<li><p><a href="https://jmhldotorg.files.wordpress.com/2013/10/papernipsworkshopbayesiandeeplearning2016.pdf">Importance weighted autoencoders with random neural network parameters</a><br /> Daniel Hernández-Lobato, Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, and Rich Turner <br /> NIPS Workshop on Bayesian Deep Learning, 2016</p>
</li>
<li><p><a href="http://approximateinference.org/accepted/BuiEtAl2016.pdf">Black-box alpha divergence for generative models</a><br /> Thang Bui, Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, and Rich Turner <br /> NIPS Workshop on Advances in Approximate Bayesian Inference, 2016</p>
</li>
<li><p><a href="http://approximateinference.org/accepted/TebbuttEtAl2016.pdf">Circular Pseudo-point approximations for scaling Gaussian processes</a><br /> Will Tebbutt, Thang Bui and Rich Turner <br /> NIPS Workshop on Advances in Approximate Bayesian Inference, 2016</p>
</li>
<li><p>Bayesian Gaussian process state space models via Power-EP <br /> Thang Bui, Carl Rasmussen and Rich Turner <br />  ICML Workshop on Data efficient Machine Learning, 2016</p>
</li>
<li><p><a href="https://arxiv.org/abs/1511.03405">Training deep Gaussian processes using stochastic expectation propagation and probabilistic backpropagation</a><br /> Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato and Rich Turner <br /> NIPS Workshop on Advances in Approximate Bayesian Inference, 2015</p>
</li>
<li><p><a href="https://www.blackboxworkshop.org/pdf/gplvm_blackbox_final.pdf">Stochastic variational inference for Gaussian process latent variable models using back constraints</a><br /> Thang Bui and Rich Turner <br /> NIPS Workshop on Black Box Learning and Inference, 2015</p>
</li>
<li><p><a href="https://www.blackboxworkshop.org/pdf/nips2015_blackbox_alpha.pdf">Black-box alpha-divergence minimisation</a><br /> José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Thang Bui and Rich Turner <br /> NIPS Workshops on Advances in Approximate Bayesian Inference and Black Box Learning and Inference, 2015.</p>
</li>
<li><p><a href="https://arxiv.org/abs/1511.03249">Stochastic expectation propagation for large scale Gaussian process classification</a><br /> Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, Thang Bui and Rich Turner <br /> NIPS Workshop on Advances in Approximate Bayesian Inference, 2015.</p>
</li>
<li><p><a href="http://learning.eng.cam.ac.uk/pub/Public/Turner/Publications/Tobar_Bui_Turner_NIPS15_TS.pdf">Design of covariance functions using inter-domain inducing variables</a><br /> Felipe Tobar, Thang Bui and Rich Turner<br />  NIPS Workshop on Time Series, 2015<br /> <b>Best paper award</b></p>
</li>
</ul>
<h2>Misc</h2>
<ul>
<li><p><a href="docs/reports/tvo_annealed_is.pdf">Connecting the Thermodynamic Variational Objective and Annealed Importance Sampling</a><br /> Thang Bui</p>
</li>
<li><p><a href="docs/reports/tr_sparseNonConj.pdf">Sparse Approximations for Non-Conjugate Gaussian Process Regressions</a><br /> Thang Bui and Rich Turner</p>
</li>
</ul>
<h2>Contact</h2>
<p>thang.buivn at at at at gmail.com</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-06-22 15:50:58 AEST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</div>
</body>
</html>
