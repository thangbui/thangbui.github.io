# jemdoc: addcss{style/thang.css}
= Thang Bui

~~~
I'm looking for PhD/MPhil students and a postdoc. If you have a strong background in statistics and computer science, feel free to get in touch. Please see my research interests and publications below, and my email address at the bottom of this page.
~~~

== Bio
I'm a lecturer (equivalent to tenure-track Assistant Professor) in Machine Learning at the [https://comp.anu.edu.au/ School of Computing], [https://www.anu.edu.au/ Australian National University] since July 2022.\n

Before that, I was a lecturer at the University of Sydney from 2018 to 2022 and spent two years (2019-2020) at Uber AI.\n

I completed my doctoral training at the [http://mlg.eng.cam.ac.uk Cambridge Machine Learning group], supervised by [http://cbl.eng.cam.ac.uk/Public/Turner/WebHome Richard Turner] and advised by [http://learning.eng.cam.ac.uk/carl/ Carl Rasmussen].\n

My current research interests include probabilistic modelling and inference, Monte Carlo and approximate inference methods, distributed, active and continual leaning, and model-based reinforcement learning.\n


== News
- 7/2022: I joined ANU as a tenure-track faculty member.
- 2/2022: New work on Bayesian federated learning, led by Matt Ashman is now available on [https://arxiv.org/abs/2202.12275 arxiv].
- 2022: I'm an area chair for NeurIPS 2022 and an action editor for TMLR.
- 2021: I was awarded an unrestricted grant from Facebook.
#- 10/2017: Our work on unifying GP approximations using Power-EP has been accepted to JMLR.
#- 09/2017: Streaming approximations for Gaussian process regression and classification [https://arxiv.org/abs/1705.07131 arxiv] accepted to NIPS 2017.
#- 08/2017: I will present our work on online GP inference and learning at the [http://roseyu.com/time-series-workshop/ ICML Time Series workshop] in Sydney.
#- 03-current/2017: I am working on [http://github.com/thangbui/geepee geepee], a package that can handle sparse deterministic approximations (using Power-EP, variational free-energy, and approximate Power-EP aka. black-box alpha) for Gaussian process models (including regression, classification, latent variable models, state space models and deep GPs).
#- 06/2017 Current highlight: Streaming approximations for Gaussian process regression and classification [https://arxiv.org/abs/1705.07131 arxiv].
#- 12-2016 Three extended abstracts have been accepted to NIPS workshops on approximate Bayesian inference and Bayesian deep learning.
#- 06-09/2016 Internship @ Google (Machine Intelligence)
#- 06/2016 Current highlight: Power-EP for sparse Gaussian processes for regression, classification, latent variable models [https://arxiv.org/abs/1605.07066 arxiv].
#- 04/2016 Two papers accepted to ICML [http://arxiv.org/abs/1602.04133 Deep GPs] and [http://arxiv.org/abs/1511.03243 Black-box \alpha] + one workshop paper on Power-EP for GP state space models. 
#- 03/2016 Current highlight: Deep Gaussian processes for regression [http://arxiv.org/abs/1602.04133 arxiv].
#- 12-2015 Our work on Design of covariance functions using inter-domain inducing variables won the *Best Paper award* at [https://sites.google.com/site/nipsts2015/home the NIPS time series workshop]
#- 11-2015 Five extended abstracts have been accepted to NIPS workshops on approximate Bayesian inference, black-box inference and time series. These are joint works with these awesome collaborators [http://jmhl.org/ José Miguel Hernández-Lobato], [http://dhnzl.org/ Daniel Hernández-Lobato], [http://yingzhenli.net/home/en/ Yingzhen Li], [http://www.cmm.uchile.cl/?cmm_people=felipe-tobar Felipe Tobar] and [http://cbl.eng.cam.ac.uk/Public/Turner/Turner Rich Turner].
#- 09-2015 I helped Felipe Tobar and Rich with the experiments of this paper /Learning stationary time series using Gaussian Processes with nonparametric kernels/ which has been accepted to NIPS 2015 with *spotlight* presentation 
#- 06/2015 I am a recipient of the [http://googleresearch.blogspot.co.uk/2015/06/announcing-2015-google-european.html 2015 Google European Doctoral Fellowship]
#- 09-2014 /Tree structured Gaussian Process Approximations/ paper accepted to NIPS 2014 with *spotlight* presentation
#- 05-2014 [http://cbl.eng.cam.ac.uk/Public/Turner/Turner Rich Turner] and I gave a talk on Advanced GP approximations at CUED\n
#- 04-2014 Attended the [http://mlss2014.hiit.fi/ MLSS] in Reykjavik, Iceland\n
#- 02-2014 [http://mlg.eng.cam.ac.uk/frellsen/ Jes Frellsen] and I gave a talk on Sequential Monte Carlo methods at CUED\n
#- 01-2014 Attended the [http://ml.dcs.shef.ac.uk/gpss/gpws14/ Gaussian Process Winter School] in Sheffield\n
#- 10-2013 Joined MLG, University of Cambridge

== Preprints
- [https://arxiv.org/abs/2202.12275 Partitioned Variational Inference: A Framework for Probabilistic Federated Learning]\nMatthew Ashman, Thang Bui, Cuong Nguyen, Stratis Markou, Adrian Weller, Siddharth Swaroop, Richard Turner\nNote: An old version of this is available [https://arxiv.org/abs/1811.11206 here].

== Conference and journal papers
- q-Paths: Generalizing the geometric annealing path using power means\n Vaden Masrani, Rob Brekelmans, Thang Bui, Frank Nielsen , Aram Galstyan, Greg Ver Steeg, and Frank Wood \n UAI 2021
- [https://arxiv.org/abs/2006.05468 Variational autoregressive Gaussian processes for continual learning]\n Sanyam Kapoor, Theofanis Karaletsos, and Thang Bui \n ICML 2021 \n [https://github.com/uber-research/vargp code]
- [https://arxiv.org/abs/2002.04033 Hierarchical Gaussian process priors for Bayesian neural network weights]\n Theofanis Karaletsos and Thang Bui \n NeurIPS 2020
- [https://arxiv.org/abs/1710.10628 Variational continual learning]\n Cuong Nguyen, Yingzhen Li, Thang Bui, and Rich Turner, \n ICLR 2018 \n [https://github.com/nvcuong/variational-continual-learning code]
- [https://arxiv.org/abs/1703.04818 Neural graph learning: Training neural networks using graphs]\n Thang Bui, Sujith Ravi, and Vivek Ramavajjala \n WSDM 2018
- [https://arxiv.org/abs/1605.07066 A unifying framework for sparse Gaussian process approximations using Power Expectation Propagation]\n Thang Bui, Josiah Yan, Rich Turner \n JMLR 2017 \n [https://github.com/thangbui/sparseGP_powerEP/ code]
- [https://arxiv.org/abs/1705.07131 Streaming sparse Gaussian process approximations]\n Thang Bui, Cuong Nguyen, and Rich Turner \n NIPS 2017 \n [https://github.com/thangbui/streaming_sparse_gp code]
- [http://jmlr.org/proceedings/papers/v48/bui16.pdf Deep Gaussian processes for regression using approximate Expectation Propagation]\n Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, and Rich Turner \n ICML 2016 \n [https://github.com/thangbui/deepGP_approxEP/ code]
- [http://jmlr.org/proceedings/papers/v48/hernandez-lobatob16.pdf  Black-box alpha-divergence minimization]\n José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang Bui, and Rich Turner \n ICML 2016 \n [https://bitbucket.org/jmh233/code_black_box_alpha_icml_2016 code]
- [https://papers.nips.cc/paper/5772-learning-stationary-time-series-using-gaussian-processes-with-nonparametric-kernels Learning stationary time series using Gaussian processes with nonparametric kernels] \n Felipe Tobar, Thang Bui, and Rich Turner \n NIPS 2015 (*Spotlight*, acceptance rate = 3.6%)
- [https://papers.nips.cc/paper/5459-tree-structured-gaussian-process-approximations Tree-structured Gaussian process approximations]\n Thang Bui and Rich Turner \n NIPS 2014 (*Spotlight*, acceptance rate = 3.6%) \n [https://github.com/thangbui/tsgp/ code]

== Thesis
- [docs/papers/thesis-thang.pdf Efficient Deterministic Approximate Bayesian Inference for Gaussian Process Models] \n PhD thesis \n University of Cambridge \n [docs/papers/thesis-thang-typos.txt typos in the print version] 

== Workshop papers
- [https://arxiv.org/abs/2012.07823 Annealed importance sampling with q-paths]\n Rob Brekelmans, Vaden Masrani, Thang Bui, Frank Wood, Aram Galstyan, Greg Ver Steeg, and Frank Nielsen, \n NeurIPS workshop on Deep Learning through Information Geometry, 2020\n *Best paper award*
- [https://arxiv.org/abs/2006.05468 Variational autoregressive Gaussian processes for continual learning] \n Sanyam Kapoor, Theofanis Karaletsos, and Thang Bui \n ICML workshop on Continual Learning, 2020
- [https://arxiv.org/abs/2002.04033 Gaussian process meta-representations for hierarchical neural network priors]\n Theofanis Karaletsos and Thang Bui \n 2nd Symposium on Advances in Approximate Bayesian Inference, 2019
- [https://arxiv.org/abs/1811.11206 Partitioned variational inference for federated Bayesian deep learning]\n Thang Bui, Cuong Nguyen, Siddharth Swaroop, and Rich Turner \n NeurIPS Bayesian Deep Learning Workshop, 2018
- [https://arxiv.org/abs/1905.02099 Understanding and improving variational continual learning]\n Siddharth Swaroop, Cuong Nguyen, Thang Bui, and Rich Turner \n NeurIPS Continual Learning Workshop, 2018
- [https://hanna-tseran.github.io/files/NeurIPS_Continual_Learning_Workshop_2018_Paper.pdf Natural variational continual learning]\n Hanna Tseran, Emtiyaz Khan, Tatsuya Harada and Thang Bui \n NeurIPS Continual Learning Workshop, 2018
- [https://arxiv.org/abs/1710.10628 Variational continual learning for deep models]\n Cuong Nguyen, Yingzhen Li, Thang Bui, and Rich Turner \n NIPS Bayesian Deep Learning Workshop, 2017
- [http://roseyu.com/time-series-workshop/submissions/TSW2017_paper_7.pdf Online variational Bayesian inference: Algorithms for sparse Gaussian processes and theoretical bounds]\n Cuong Nguyen, Thang Bui, Yingzhen Li, and Rich Turner \n ICML Time Series Workshop, 2017
- [https://jmhldotorg.files.wordpress.com/2013/10/papernipsworkshopbayesiandeeplearning2016.pdf Importance weighted autoencoders with random neural network parameters]\n Daniel Hernández-Lobato, Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, and Rich Turner \n NIPS Workshop on Bayesian Deep Learning, 2016
- [http://approximateinference.org/accepted/BuiEtAl2016.pdf Black-box alpha divergence for generative models]\n Thang Bui, Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, and Rich Turner \n NIPS Workshop on Advances in Approximate Bayesian Inference, 2016
- [http://approximateinference.org/accepted/TebbuttEtAl2016.pdf Circular Pseudo-point approximations for scaling Gaussian processes]\n Will Tebbutt, Thang Bui and Rich Turner \n NIPS Workshop on Advances in Approximate Bayesian Inference, 2016
- Bayesian Gaussian process state space models via Power-EP \n Thang Bui, Carl Rasmussen and Rich Turner \n  ICML Workshop on Data efficient Machine Learning, 2016
- [https://arxiv.org/abs/1511.03405 Training deep Gaussian processes using stochastic expectation propagation and probabilistic backpropagation]\n Thang Bui, José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato and Rich Turner \n NIPS Workshop on Advances in Approximate Bayesian Inference, 2015
- [https://www.blackboxworkshop.org/pdf/gplvm_blackbox_final.pdf Stochastic variational inference for Gaussian process latent variable models using back constraints]\n Thang Bui and Rich Turner \n NIPS Workshop on Black Box Learning and Inference, 2015
- [https://www.blackboxworkshop.org/pdf/nips2015_blackbox_alpha.pdf Black-box alpha-divergence minimisation]\n José Miguel Hernández-Lobato, Yingzhen Li, Daniel Hernández-Lobato, Thang Bui and Rich Turner \n NIPS Workshops on Advances in Approximate Bayesian Inference and Black Box Learning and Inference, 2015.
- [https://arxiv.org/abs/1511.03249 Stochastic expectation propagation for large scale Gaussian process classification]\n Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Yingzhen Li, Thang Bui and Rich Turner \n NIPS Workshop on Advances in Approximate Bayesian Inference, 2015.
- [http://learning.eng.cam.ac.uk/pub/Public/Turner/Publications/Tobar_Bui_Turner_NIPS15_TS.pdf Design of covariance functions using inter-domain inducing variables]\n Felipe Tobar, Thang Bui and Rich Turner\n  NIPS Workshop on Time Series, 2015\n *Best paper award*


== Misc
- [docs/reports/tvo_annealed_is.pdf Connecting the Thermodynamic Variational Objective and Annealed Importance Sampling]\n Thang Bui
- [docs/reports/tr_sparseNonConj.pdf Sparse Approximations for Non-Conjugate Gaussian Process Regressions]\n Thang Bui and Rich Turner

#== Talks
#- Thang Bui, Learning by learning deep generative models, CUED
#- Thang Bui, Deep GPs, University of Sheffield, Google DeepMind, OpenAI.
#- Thang Bui, Alex Navarro, Richard Turner, Deep Gaussian Processes, CUED.
#- [http://www-personal.acfr.usyd.edu.au/rmca4617/ Rowan McAllister], Thang Bui, State Space Abstraction for Reinforcement Learning, CUED. [docs/talks/rcc_rl.pdf part_1]
#- Thang Bui, [http://cbl.eng.cam.ac.uk/Public/Turner/Turner Richard Turner], Tree-structured Gaussian Process approximations, CUED. [docs/talks/mlg_tsgptalk.pdf slides] [docs/talks/cbl_tsgptalk.pdf more slides]
#- [http://cbl.eng.cam.ac.uk/Public/Turner/Turner Richard Turner], Thang Bui, Advanced GP approximations, CUED. [docs/talks/rcc_vargp.pdf part_1]
#- [http://mlg.eng.cam.ac.uk/frellsen/ Jes Frellsen], Thang Bui, Introduction to Sequential Monte Carlo methods, CUED. [docs/talks/rcc_smc.pdf slides]


#== Short bio
#I received a Bachelor of Engineering (Telecommunications) from [http://www.adelaide.edu.au Adelaide University], Australia in 2011. I then spent sometime working as a research associate at [http://www.trc.adelaide.edu.au Teletraffic Research Centre] and [http://telari.com.au Telari], Adelaide. I did several internships in previous summers at Google Research in 2016, [http://www.toshiba.eu/eu/Cambridge-Research-Laboratory Toshiba Cambridge Research Lab] in 2014, [http://www.cisra.com.au CISRA, Canon Australia] in 2011-2012, [http://www.unsw.edu.au UNSW] in 2010-2011 and [http://www.adelaide.edu.au UofA] in 2009-2010.

#== Other things
# Member of [http://www.trin.cam.ac.uk Trinity College]

# Reviewer for JMLR (2016, 2017, 2018), NIPS (2016-20), ICLR (2017-21), ICML (2017-20), AISTATS (2018-21), UAI (2018) and various NIPS and ICML workshops

== Contact
Hanna Neumann Building 145, Office 2.22\n
School of Computing, College of Engineering and Computer Science\n
The Australian National University, Australia\n
thang.bui at at at at anu.edu.au


